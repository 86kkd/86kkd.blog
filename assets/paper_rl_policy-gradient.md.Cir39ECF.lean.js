import{_ as a,c as t,a2 as i,o}from"./chunks/framework.C3rjE0BJ.js";const u=JSON.parse('{"title":"policy-gradient","description":"","frontmatter":{},"headers":[],"relativePath":"paper/rl/policy-gradient.md","filePath":"paper/rl/policy-gradient.md","lastUpdated":1734501116000}'),r={name:"paper/rl/policy-gradient.md"};function l(n,e,s,c,p,d){return o(),t("div",null,e[0]||(e[0]=[i('<h1 id="policy-gradient" tabindex="-1">policy-gradient <a class="header-anchor" href="#policy-gradient" aria-label="Permalink to &quot;policy-gradient&quot;">​</a></h1><h2 id="what-is-policy-gradient" tabindex="-1">What is Policy-gradient <a class="header-anchor" href="#what-is-policy-gradient" aria-label="Permalink to &quot;What is Policy-gradient&quot;">​</a></h2><p>Policy-gradient methods can learn a stochastic policy while value functions can’t.</p><p>This has two consequences:</p><ul><li><p>We don’t need to implement an exploration/exploitation trade-off by hand. Since we output a probability distribution over actions, the agent explores the state space without always taking the same trajectory.</p></li><li><p>We also get rid of the problem of perceptual aliasing. Perceptual aliasing is when two states seem (or are) the same but need different actions.</p></li></ul><div class="info custom-block github-alert"><p class="custom-block-title">INFO</p><p><code>perceptual aliasing</code> 是因为value based的方式对于一个状态的最优解是固定的, 所以导致模型无法很好的处理要求对一个状态进行随机做出的决策。</p></div>',6)]))}const g=a(r,[["render",l]]);export{u as __pageData,g as default};
