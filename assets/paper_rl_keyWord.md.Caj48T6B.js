import{_ as e,c as r,a2 as t,o}from"./chunks/framework.C3rjE0BJ.js";const f=JSON.parse('{"title":"专业名词汇总","description":"","frontmatter":{},"headers":[],"relativePath":"paper/rl/keyWord.md","filePath":"paper/rl/keyWord.md","lastUpdated":1731930086000}'),l={name:"paper/rl/keyWord.md"};function d(i,a,s,n,p,c){return o(),r("div",null,a[0]||(a[0]=[t('<h1 id="专业名词汇总" tabindex="-1">专业名词汇总 <a class="header-anchor" href="#专业名词汇总" aria-label="Permalink to &quot;专业名词汇总&quot;">​</a></h1><p>本markdown汇总了深度强化学习的专业名词和概念,所有内容仅个人观点，如有问题欢迎指正</p><h2 id="off-policy" tabindex="-1">Off-policy <a class="header-anchor" href="#off-policy" aria-label="Permalink to &quot;Off-policy&quot;">​</a></h2><p>基于Off-policy based 方法可以从其他策略(如随机策略)生成的经验中学习,而不必严格遵循正在学习的策略。 这使得它能够更灵活地利用历史数据或探索性行为。</p><h2 id="value-based" tabindex="-1">Value-based <a class="header-anchor" href="#value-based" aria-label="Permalink to &quot;Value-based&quot;">​</a></h2><p>Value-based的方法关注的是学习动作价值函数,而不是直接学习策略。该函数估计在给定状态下采取特定动作的长期回报。</p><h2 id="时序差分-td" tabindex="-1">时序差分(TD) <a class="header-anchor" href="#时序差分-td" aria-label="Permalink to &quot;时序差分(TD)&quot;">​</a></h2><p>这是一种结合了蒙特卡洛方法和动态规划思想的学习方法。 TD方法通过估计的即时奖励和下一状态的估计值来更新当前状态的估计值,而不需要等待整个回合结束。</p><h2 id="动作价值函数" tabindex="-1">动作价值函数 <a class="header-anchor" href="#动作价值函数" aria-label="Permalink to &quot;动作价值函数&quot;">​</a></h2>',9)]))}const u=e(l,[["render",d]]);export{f as __pageData,u as default};
